{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d890819",
   "metadata": {},
   "source": [
    "# **Theoritical >>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5f5a6",
   "metadata": {},
   "source": [
    "### 1. ***What is K-Nearest Neighbors (KNN) and how does it work ?***\n",
    "*Answer-*\n",
    "\n",
    "**`K-Nearest Neighbors (KNN)`** is a simple, intuitive, and widely used **`supervised machine learning algorithm`** used for **`classification`** and **`regression`** tasks. It works based on the idea that similar data points are close to each other in feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### **How KNN Works:**\n",
    "\n",
    "1. **Store Training Data**:  \n",
    "   KNN is a **lazy learner**, meaning it doesn’t do any learning or model building during the training phase. It just stores the training data.\n",
    "\n",
    "2. **Calculate Distance**:  \n",
    "   When a new (unseen) data point needs to be classified or predicted, KNN calculates the **distance** between this point and all points in the training dataset. Common distance metrics:\n",
    "   - **Euclidean distance** (most common)\n",
    "   - Manhattan distance\n",
    "   - Minkowski distance\n",
    "\n",
    "3. **Find Neighbors (K)**:  \n",
    "   Select the **K** nearest data points (neighbors) to the query point based on the distance.\n",
    "\n",
    "4. **Make Prediction**:\n",
    "   - **Classification**: The majority label among the K neighbors becomes the prediction.\n",
    "   - **Regression**: The average (or weighted average) of the values of K neighbors is used.\n",
    "\n",
    "---\n",
    "\n",
    "###  Example:\n",
    "Let’s say you have data on fruits with features like weight and color, and you want to classify a new fruit.\n",
    "\n",
    "- Pick a value for **K**, say `K=3`.\n",
    "- Find the 3 fruits in your dataset that are closest to the new fruit.\n",
    "- See what class those 3 belong to (e.g., apple, orange, banana).\n",
    "- The most common class among them is your prediction.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Important Considerations:**\n",
    "- **Choosing K**:\n",
    "  - Too small (e.g., K=1): Sensitive to noise.\n",
    "  - Too large: Can dilute local structure.\n",
    "  - Usually chosen via cross-validation.\n",
    "\n",
    "- **Feature Scaling**:\n",
    "  - Since KNN relies on distance, it’s important to normalize or standardize features (like using Min-Max or Z-score scaling).\n",
    "\n",
    "- **Computational Cost**:\n",
    "  - Can be slow for large datasets since it computes distance to every point during prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Pros:\n",
    "- Easy to understand and implement.\n",
    "- No assumptions about data distribution.\n",
    "- Works well with small to medium-sized datasets.\n",
    "\n",
    "### ❌ Cons:\n",
    "- Slower with large datasets.\n",
    "- Performance depends heavily on distance metric and feature scaling.\n",
    "- Doesn’t work well with high-dimensional data (curse of dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d420a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ecf3d",
   "metadata": {},
   "source": [
    "### 2. ***What is the difference between KNN Classification and KNN Regression***\n",
    "*Answer-*\n",
    "\n",
    "Great question! While **K-Nearest Neighbors (KNN)** can be used for both **classification** and **regression**, the **difference lies in how the prediction is made** based on the neighbors. Here's a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "####  **KNN Classification:**\n",
    "- **Goal**: Predict a **category** or **class label**.\n",
    "- **How it works**:\n",
    "  - Find the `K` nearest neighbors.\n",
    "  - Count how many neighbors belong to each class.\n",
    "  - Predict the class that appears **most frequently** among the neighbors (majority vote).\n",
    "\n",
    "####  **Example:**\n",
    "- Suppose you're trying to classify a fruit as **apple**  or **orange.**\n",
    "- For `K = 5`, you find:\n",
    "  - 3 apples\n",
    "  - 2 oranges\n",
    "- Since \"apple\" is the majority, the model predicts: **apple**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **KNN Regression:**\n",
    "- **Goal**: Predict a **continuous value**.\n",
    "- **How it works**:\n",
    "  - Find the `K` nearest neighbors.\n",
    "  - Take the **average (or weighted average)** of their target values.\n",
    "  - Return that average as the predicted value.\n",
    "\n",
    "#### **Example:**\n",
    "- Suppose you're predicting the **price of a house**.\n",
    "- For `K = 3`, you find 3 nearest houses with prices:\n",
    "  - \\$200,000, \\$210,000, \\$190,000\n",
    "- Predicted price = **average = \\$200,000**\n",
    "\n",
    "---\n",
    "\n",
    "####  **Summary Table:**\n",
    "\n",
    "| Feature            | KNN Classification          | KNN Regression             |\n",
    "|--------------------|-----------------------------|----------------------------|\n",
    "| **Target**         | Discrete labels (e.g., cat/dog) | Continuous values (e.g., price, temperature) |\n",
    "| **Prediction**     | Majority class vote         | Average (or weighted average) of neighbors |\n",
    "| **Output Example** | `\"apple\"`, `\"dog\"`          | `45.2`, `199.9`            |\n",
    "| **Evaluation Metrics** | Accuracy, Precision, Recall, F1 | MSE, RMSE, MAE             |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98ce0c",
   "metadata": {},
   "source": [
    "### 3. ***What is the role of the distance metric in KNN?***\n",
    "*Answer-*\n",
    "\n",
    "In KNN (k-Nearest Neighbors), distance metrics quantify the similarity between data points, enabling the algorithm to identify the \"k\" nearest neighbors for classification or regression tasks.  \n",
    "\n",
    "**• Quantifying Similarity:**\n",
    "-   KNN, at its core, relies on the principle that data points closer to each other are more likely to belong to the same class or have similar values. Distance metrics provide a way to measure this proximity, essentially translating the concept of \"closeness\" into a numerical value.\n",
    "\n",
    "**• Finding Nearest Neighbors:**\n",
    "-   Once a distance metric is chosen, KNN calculates the distance between a new data point (the query point) and all other data points in the training set. The algorithm then identifies the \"k\" data points with the shortest distances to the query point, which are its nearest neighbors.\n",
    "\n",
    "**• Decision Making:**\n",
    "-   The class or value of the query point is then determined based on the majority class or average value of its k nearest neighbors.\n",
    "\n",
    "**• Types of Distance Metrics:**\n",
    "-   Several distance metrics can be used in KNN, including Euclidean distance (straight-line distance), Manhattan distance (sum of absolute differences), and Minkowski distance (a generalization of Euclidean and Manhattan distances). The choice of distance metric depends on the nature of the data and the problem being addressed.\n",
    "\n",
    "**• Impact on Performance:**\n",
    "-   The choice of distance metric can significantly impact the performance of the KNN algorithm. A poorly chosen metric can lead to inaccurate classifications or predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56449ee",
   "metadata": {},
   "source": [
    "### 4. ***What is the Curse of Dimensionality in KNN?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "#### **`Curse of Dimensionality?`**\n",
    "\n",
    "The **`Curse of Dimensionality`** refers to the problems that arise when working with **high-dimensional data** (i.e., data with a large number of features or variables).\n",
    "\n",
    "In the context of **KNN**, it causes trouble because as the number of dimensions increases:\n",
    "- **Data points become increasingly sparse.**\n",
    "- **Distances between points become less meaningful.**\n",
    "- **All points start to seem equally distant**, which ruins KNN's whole idea of \"nearest\" neighbors\n",
    "\n",
    "---\n",
    "\n",
    "####  Why It Matters for KNN:\n",
    "\n",
    "KNN relies **heavily** on measuring distance. But in high-dimensional spaces:\n",
    "\n",
    "- **Euclidean distance** starts to lose its discriminative power.\n",
    "- Nearest neighbors are **not much closer** than faraway points.\n",
    "- This leads to **poor performance**, overfitting, or noisy predictions.\n",
    "\n",
    "#### Example:\n",
    "Measuring distance in:\n",
    "- 2D → easy to see who’s close.\n",
    "- 1000D → our \"neighbors\" might be more like strangers… far apart in weird ways.\n",
    "\n",
    "---\n",
    "\n",
    "####  Practical Effects on KNN:\n",
    "\n",
    "| Problem                                 | Impact on KNN                                      |\n",
    "|----------------------------------------|---------------------------------------------------|\n",
    "| Sparse data distribution               | Few meaningful neighbors                          |\n",
    "| Less meaningful distance metrics       | Poor neighbor selection                           |\n",
    "| Longer computation times               | Slower predictions                                |\n",
    "| Increased overfitting risk             | Especially with small datasets                    |\n",
    "\n",
    "---\n",
    "\n",
    "####  How to Handle It:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - Use techniques like **PCA** (Principal Component Analysis), **t-SNE**, or **Autoencoders** to reduce dimensions while preserving structure.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Keep only the **most relevant features** (via correlation, importance scores, etc.)\n",
    "\n",
    "3. **Use Different Algorithms**:\n",
    "   - KNN isn’t always the best for high dimensions. Consider **tree-based** models (like Random Forests or XGBoost) or **SVM** with kernels.\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "> The Curse of Dimensionality in KNN means that as you add more features, your data spreads out, distances become unreliable, and the model loses its ability to identify truly \"nearest\" neighbors.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95320564",
   "metadata": {},
   "source": [
    "### 5. ***How can we choose the best value of K in KNN?***\n",
    "*Answer-*\n",
    "\n",
    "Choosing the **best value of K** in **K-Nearest Neighbors (KNN)** is **critical** because it directly affects the model’s accuracy and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 What Does “K” Mean Again?\n",
    "\n",
    "- `K` is the number of **neighbors** used to make a prediction.\n",
    "- Think of it as a **smoothing factor**:\n",
    "  - **Small K (e.g., 1 or 3)** → More flexible, may overfit (sensitive to noise).\n",
    "  - **Large K (e.g., 20, 50)** → Smoother predictions, may underfit (miss local patterns).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 How to Choose the Best K:\n",
    "\n",
    "#### ✅ 1. **Use of Cross-Validation** (Most Reliable Method)\n",
    "- Perform **k-fold cross-validation** for a range of K values.\n",
    "- Pick the K with the **lowest error** or **highest accuracy** on validation sets.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ 2. **Plot Error vs. K** (Elbow Method)\n",
    "- Plot K (x-axis) vs. accuracy or error (y-axis)\n",
    "- Look for the **\"elbow\" point**—where the curve flattens out, indicating diminishing returns.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ 3. **Odd Values of K** (for classification)\n",
    "- Helps avoid **ties** in voting.\n",
    "- Especially useful when the number of classes = 2.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ 4. **Domain Knowledge**\n",
    "- Sometimes the “right” K depends on the data or domain (e.g., if the class boundaries are tight, a smaller K might work better).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Summary Table:\n",
    "\n",
    "| K Value | Behavior                     | Risk              |\n",
    "|---------|------------------------------|-------------------|\n",
    "| Small   | High model flexibility        | Overfitting       |\n",
    "| Large   | Smooth, stable predictions    | Underfitting      |\n",
    "| Optimal | Found via cross-validation    | Best generalization |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82e2b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Try K from 1 to 20\n",
    "k_range = range(1, 21)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    scores.append(score.mean())\n",
    "\n",
    "best_k = k_range[np.argmax(scores)]\n",
    "print(f\"Best K: {best_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb9586",
   "metadata": {},
   "source": [
    "### 6. ***What are KD Tree and Ball Tree in KNN?***\n",
    "*Answer-*\n",
    "\n",
    "When using **K-Nearest Neighbors (KNN)** on **large datasets**, computing the distance to **every point** can be super slow. That’s where **KD Tree** and **Ball Tree** come in—they're **data structures** that help **speed up neighbor searches**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌲 KD Tree (K-Dimensional Tree)\n",
    "\n",
    "#### 🧠 What is it?\n",
    "- A **binary tree** that recursively splits the data along **axis-aligned** dimensions.\n",
    "- Each node splits the space into two parts using one feature at a time (like slicing a cake).\n",
    "- Works best for **low-dimensional** data (usually < 20 features).\n",
    "\n",
    "#### ⚙️ How it works:\n",
    "1. Choose a dimension (e.g., x, y, z...).\n",
    "2. Split the data based on the median value of that dimension.\n",
    "3. Recursively repeat for each child node.\n",
    "\n",
    "#### ✅ Pros:\n",
    "- Fast for **low dimensions**\n",
    "- Efficient for **nearest neighbor queries**\n",
    "\n",
    "#### ❌ Cons:\n",
    "- Struggles in **high-dimensional spaces** due to the Curse of Dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "### 🏐 Ball Tree\n",
    "\n",
    "#### 🧠 What is it?\n",
    "- A **tree-based structure** where data points are grouped into **hyperspheres** (balls).\n",
    "- Each node represents a \"ball\" containing a subset of points.\n",
    "- Works better for **high-dimensional** or **non-Euclidean** spaces.\n",
    "\n",
    "#### ⚙️ How it works:\n",
    "1. Organize points into clusters (balls).\n",
    "2. For each node, store the center and radius of its ball.\n",
    "3. Prune entire branches when searching by comparing distances to the balls.\n",
    "\n",
    "#### ✅ Pros:\n",
    "- Better than KD Tree for **higher dimensions**\n",
    "- More flexible with different distance metrics\n",
    "\n",
    "#### ❌ Cons:\n",
    "- Slightly slower for very low-dimensional data compared to KD Tree\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Which One Should You Use?\n",
    "\n",
    "| Scenario                             | Use KD Tree     | Use Ball Tree   |\n",
    "|--------------------------------------|------------------|------------------|\n",
    "| Low-dimensional data (<20 features)  | ✅                | ❌                |\n",
    "| High-dimensional data (20+ features) | ❌                | ✅                |\n",
    "| Euclidean distance                   | ✅                | ✅                |\n",
    "| Other distance metrics (e.g. cosine) | ❌                | ✅                |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402d889",
   "metadata": {},
   "source": [
    "### 7. ***When should you use KD Tree vs. Ball Tree?***\n",
    "*Answer-*\n",
    "\n",
    "Choosing between **KD Tree** and **Ball Tree** depends mostly on the **dimensionality of your data**, the **distance metric**, and **performance needs**.\n",
    "\n",
    "---\n",
    "\n",
    "####  TL;DR Quick Guide:\n",
    "\n",
    "| Scenario                                      | Use **KD Tree**            | Use **Ball Tree**          |\n",
    "|----------------------------------------------|-----------------------------|-----------------------------|\n",
    "| 🔢 **Low-dimensional data** (≤ ~20 features)  | ✅ Yes                      | ❌ Not ideal                |\n",
    "| 🔢 **High-dimensional data** ( > ~20 features)| ❌ Poor performance         | ✅ Better performance       |\n",
    "| 🧮 **Euclidean distance**                     | ✅ Supported                | ✅ Supported                |\n",
    "| 📐 **Other distance metrics** (e.g., cosine)  | ❌ Not supported            | ✅ More flexible            |\n",
    "| 🚀 **Faster for simple data**                 | ✅ Lightweight & fast       | ❌ Slightly slower          |\n",
    "| 🌌 **Complex clustering or sparse layout**    | ❌ Less efficient           | ✅ Handles better           |\n",
    "\n",
    "---\n",
    "\n",
    "####  Why?\n",
    "\n",
    "#### ✅ Use **KD Tree** when:\n",
    "- Your data has **few features** (e.g., 2D, 3D up to maybe 15–20).\n",
    "- You're using **Euclidean distance**.\n",
    "- You want the fastest neighbor searches in **simple, well-behaved data**.\n",
    "\n",
    "#### ✅ Use **Ball Tree** when:\n",
    "- You're dealing with **high-dimensional data**.\n",
    "- You need support for **non-Euclidean** distance metrics.\n",
    "- Your data distribution is **non-uniform or clustered**.\n",
    "- You're seeing performance degradation with KD Tree.\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-World Example:\n",
    "\n",
    "- A **2D GPS location** dataset → Use **KD Tree**\n",
    "- A **100-dimensional text embedding** (e.g., word vectors) → Use **Ball Tree**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a8742",
   "metadata": {},
   "source": [
    "### 8. ***What are the disadvantages of KNN?***\n",
    "*Answer-*\n",
    "\n",
    "While **K-Nearest Neighbors (KNN)** is simple and intuitive, it **definitely has some drawbacks**—especially when applied carelessly or on the wrong kind of data.\n",
    "\n",
    "---\n",
    "\n",
    "#### ❌ **Disadvantages of KNN**:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 🚀 **Slow Predictions (Computationally Expensive)**\n",
    "- **KNN is lazy**: it does no work during training, but **a lot during prediction**.\n",
    "- For every new input, it computes distance to **every point** in the training set.\n",
    "- Bad news when the dataset is large: O(n × d) time per prediction (n = number of samples, d = number of features).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 📐 **Sensitive to the Choice of Distance Metric**\n",
    "- The model’s performance depends **heavily** on how you measure “closeness.”\n",
    "- Euclidean, Manhattan, Cosine, etc., can produce **very different results**.\n",
    "- No one-size-fits-all: you often have to test different metrics.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 🧪 **Curse of Dimensionality**\n",
    "- In high-dimensional spaces, all points tend to look **equally far apart**.\n",
    "- Makes it hard for KNN to find \"true\" nearest neighbors.\n",
    "- Distance becomes less meaningful → **accuracy drops**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. ⚖️ **Needs Feature Scaling**\n",
    "- If the features aren’t on the same scale, distance calculations become **skewed**.\n",
    "- Example: age (0–100) and income (0–100,000) → income dominates.\n",
    "- It **must** use normalization or standardization (e.g., MinMaxScaler or StandardScaler).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 🔊 **Sensitive to Noise and Outliers**\n",
    "- A single noisy data point can lead to **misclassification**, especially with small `K`.\n",
    "- No built-in mechanism for dealing with noisy labels or outlier values.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. 🧠 **Memory-Intensive**\n",
    "- KNN **stores the entire training set**.\n",
    "- This can consume a lot of **RAM** for big datasets.\n",
    "- Not ideal for edge devices or memory-constrained environments.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. ⚙️ **Hard to Interpret K Choice**\n",
    "- Choosing the right `K` is not always straightforward.\n",
    "- Too small: overfitting  \n",
    "- Too large: underfitting\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. 🔀 **Poor for Imbalanced Data**\n",
    "- If one class dominates the training set, it’s likely to dominate predictions as well.\n",
    "- Needs strategies like **class weighting** or **distance-weighted voting** to fix this.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📦 Summary Table:\n",
    "\n",
    "| Disadvantage                     | Why It’s a Problem                      |\n",
    "|----------------------------------|-----------------------------------------|\n",
    "| Slow with large datasets         | Must compute distances to all points    |\n",
    "| Needs feature scaling            | Uneven scales distort results           |\n",
    "| Struggles with high dimensions   | Distances become meaningless            |\n",
    "| Sensitive to noise/outliers      | Can lead to incorrect predictions       |\n",
    "| Poor with imbalanced datasets    | Biased toward the majority class        |\n",
    "| No model interpretation          | Not easily explainable                  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92e712",
   "metadata": {},
   "source": [
    "### 9. ***How does feature scaling affect KNN?***\n",
    "*Answer-*\n",
    "Feature scaling is crucial for **K-Nearest Neighbors (KNN).** Since Knn relies entirely on distance calculations, unscaled features can completely throw off the results.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "####  Why Feature Scaling Matters in KNN:\n",
    "\n",
    "KNN uses **distance metrics** like **Euclidean** or **Manhattan** to measure closeness. If your features are on **different scales**, then features with **larger numerical ranges will dominate** the distance calculation.\n",
    "\n",
    "####  Example:\n",
    "| Feature     | Scale         |\n",
    "|-------------|----------------|\n",
    "| Age         | 0–100          |\n",
    "| Income ($)  | 0–100,000      |\n",
    "\n",
    "In a distance formula, \"Income\" will **overpower** \"Age\", even if \"Age\" is more relevant for prediction. That leads to **biased or incorrect neighbors**.\n",
    "\n",
    "---\n",
    "\n",
    "####  Example (Euclidean Distance):\n",
    "\\[\n",
    "\\text{Distance} = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}\n",
    "\\]\n",
    "\n",
    "If `x1 = age` (0–100) and `x2 = income` (0–100000), the second term can **completely dominate**, making \"age\" meaningless in the calculation.\n",
    "\n",
    "---\n",
    "\n",
    "####  Common Scaling Techniques:\n",
    "\n",
    "#### 1. **Min-Max Scaling (Normalization)**\n",
    "- Scales all features to a 0–1 range.\n",
    "- Preserves the shape of the distribution.\n",
    "\n",
    "#### 2. **Standardization (Z-score scaling)**\n",
    "- Centers data to mean = 0 and std = 1.\n",
    "- Good when features follow a normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "####  Which One Should You Use?\n",
    "\n",
    "| Scenario                        | Use This Scaling       |\n",
    "|----------------------------------|-------------------------|\n",
    "| Features are bounded (0–100)     | Min-Max Scaling         |\n",
    "| Features have outliers           | Standardization         |\n",
    "| You're using KNN or SVM          | Either works (try both) |\n",
    "| Sparse or skewed data            | Consider RobustScaler   |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 TL;DR:\n",
    "\n",
    "> 📏 **Feature scaling ensures all features contribute equally** to distance calculations in KNN. Without it, features with larger scales will dominate, leading to poor accuracy and bad neighbors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f814b9",
   "metadata": {},
   "source": [
    "### 10 & 11. ***What is PCA (Principal Component Analysis)? How does it work?***\n",
    "*Answer-*\n",
    "\n",
    "**`PCA** is a **dimensionality reduction`** technique. It transforms a dataset with many features (possibly correlated) into a **smaller set of new features** called **principal components**— without losing too much of the original information.\n",
    "\n",
    "-   Fewer features (lower dimensions).\n",
    "-   Uncorrelated components.\n",
    "-   Faster computation in models like KNN, SVM.\n",
    "-   Easier visualization (2D or 3D)\n",
    "---\n",
    "\n",
    "####  **Why use PCA?**\n",
    "\n",
    "- To **reduce complexity** in high-dimensional data  \n",
    "- To **remove redundancy** (correlated features)  \n",
    "- To **speed up** algorithms like KNN, SVM, etc.  \n",
    "- For **visualization** of data in 2D or 3D  \n",
    "\n",
    "---\n",
    "\n",
    "####  **How PCA Works (Step-by-Step)**\n",
    "\n",
    "1. **Standardize the Data**\n",
    "   - Mean = 0, standard deviation = 1\n",
    "   - PCA is sensitive to scale, so this step is crucial\n",
    "\n",
    "2. **Compute the Covariance Matrix**\n",
    "   - Measures how features vary together\n",
    "\n",
    "3. **Calculate Eigenvalues and Eigenvectors**\n",
    "   - Eigenvectors = directions (principal components)\n",
    "   - Eigenvalues = importance (variance explained by each component)\n",
    "\n",
    "4. **Sort & Select Top Components**\n",
    "   - Keep only the first `k` components that explain the most variance (e.g., 95%)\n",
    "\n",
    "5. **Transform the Data**\n",
    "   - Project the original data onto the selected components\n",
    "\n",
    "---\n",
    "\n",
    "####  Simple Analogy:\n",
    "\n",
    "If a 3D object (like, a cloud of data). PCA finds the **best 2D angle** from which to view it, so that we still see the most **shape and structure**, but with fewer dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "####  Visualization:\n",
    "\n",
    "Original (3D):  \n",
    "🟢🔴🔵\n",
    "\n",
    "PCA (2D):  \n",
    "🟢🔴 (still tells most of the story)\n",
    "\n",
    "---\n",
    "\n",
    "####  TL;DR:\n",
    "\n",
    "> **PCA** reduces the number of features by creating new ones (principal components) that retain the **most important variance** in the data—making your models faster, simpler, and sometimes more accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836673e",
   "metadata": {},
   "source": [
    "### 12. ***What is the geometric intuition behind PCA?***\n",
    "*Answer-*\n",
    "\n",
    "The **geometric intuition** behind **PCA (Principal Component Analysis)** is what really makes the concept *click*.\n",
    "\n",
    "---\n",
    "\n",
    "#### PCA in Simple Terms:\n",
    "\n",
    "If my data is a **cloud of points** in space. PCA finds a **new set of axes** (directions) that:\n",
    "-  Point along the directions of **maximum spread (variance)** in the data.\n",
    "-  Remove redundancy (like overlapping axes or correlated features).\n",
    "-  Let us drop the least useful dimensions (those with the least variance) while preserving the **essence** of the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2D Example (Easy to Visualize):\n",
    "\n",
    "You have 2D data like this:\n",
    "\n",
    "```\n",
    "⬤       ⬤\n",
    "    ⬤       ⬤\n",
    "⬤       ⬤\n",
    "```\n",
    "\n",
    "Now drawing a **line** through the longest direction of the spread. That’s **Principal Component 1 (PC1)**. It captures the most **variance** in the data.\n",
    "\n",
    "Then, drawing another line **perpendicular** to it. That’s **Principal Component 2 (PC2)**—it captures the remaining variance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Geometric Steps:\n",
    "\n",
    "1. **Center the data** (move it so the mean is at the origin).\n",
    "2. **Rotate the coordinate axes** so they line up with the directions of **maximum variance**.\n",
    "3. The **first axis** (PC1) captures the most \"spread\".\n",
    "4. Now **dropping the axes** (PC2, PC3...) with less variance—this is **dimensionality reduction**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Visually:\n",
    "\n",
    "#### Before PCA (Original Axes):\n",
    "\n",
    "```\n",
    "x-axis ➝\n",
    "y-axis ↑\n",
    "         ⬤\n",
    "     ⬤     ⬤\n",
    "  ⬤    ⬤\n",
    "     ⬤\n",
    "```\n",
    "\n",
    "#### After PCA (New Rotated Axes):\n",
    "\n",
    "```\n",
    "PC2 ↑\n",
    "     \\\n",
    "      \\     ⬤\n",
    "       \\ ⬤     ⬤\n",
    "        \\   ⬤\n",
    "         \\⬤\n",
    "          ➝ PC1\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "####  What PCA Is Doing Geometrically:\n",
    "\n",
    "- Finding the **directions (vectors)** that explain the most variance.\n",
    "- **Rotating** the entire coordinate system to align with those directions.\n",
    "- Then, if needed, **projecting** your data onto the most important axes (e.g., top 2 or 3).\n",
    "\n",
    "---\n",
    "\n",
    "####  Real-World Analogy:\n",
    "\n",
    "Taking a photo of a 3D object (say, a banana 🍌). If we want the best angle where we will capture **most of the shape** in **2D**.\n",
    "\n",
    "PCA finds that angle and gives you a **flattened image** that keeps **most of the info**.\n",
    "\n",
    "---\n",
    "\n",
    "####  Summary:\n",
    "\n",
    "| Term                 | Geometric Meaning                                |\n",
    "|----------------------|--------------------------------------------------|\n",
    "| Principal Component  | A new axis/direction in feature space            |\n",
    "| Variance             | The \"spread\" of data along that axis             |\n",
    "| Eigenvector          | The direction of a principal component           |\n",
    "| Projection           | Dropping the data onto a lower-dimensional space |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618e5db",
   "metadata": {},
   "source": [
    "### 13. ***What is the difference between Feature Selection and Feature Extraction?***\n",
    "*Answer-*\n",
    "\n",
    "**Feature Selection** vs. **Feature Extraction** is a key distinction in machine learning! Both are techniques for **dimensionality reduction**, but they go about it in **very different ways**.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Quick Summary:**\n",
    "\n",
    "|                     | **Feature Selection**                             | **Feature Extraction**                             |\n",
    "|---------------------|---------------------------------------------------|----------------------------------------------------|\n",
    "|  **What it does** | Chooses a **subset of existing features**         | Creates **new features** by combining old ones     |\n",
    "|  **How**          | Keeps most relevant/original features             | Transforms data to a new feature space             |\n",
    "|  **Example**      | Keep only Age, Salary, and Gender                 | PCA, LDA, autoencoders → generate new dimensions   |\n",
    "|  **Interpretability** | High (uses original features)                 | Lower (new features are abstract/compressed)       |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature Selection (Keep the Best)**\n",
    "\n",
    "We're saying:  \n",
    "> “Let me **keep the features** that matter the most and drop the rest.”\n",
    "\n",
    "####  Common Methods:\n",
    "- **Filter methods**: correlation, variance threshold\n",
    "- **Wrapper methods**: RFE (Recursive Feature Elimination)\n",
    "- **Embedded methods**: feature importance from models (e.g., decision trees, Lasso)\n",
    "\n",
    "####  Example:\n",
    "From `[Age, Height, Weight, Salary, City]`, we decide only `[Age, Salary]` are useful.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature Extraction (Create New Ones)**\n",
    "\n",
    "We're saying:  \n",
    "> “Let me **build new features** that capture the most info in a compressed way.”\n",
    "\n",
    "####  Common Methods:\n",
    "- **PCA** (Principal Component Analysis)\n",
    "- **LDA** (Linear Discriminant Analysis)\n",
    "- **t-SNE**, **UMAP** (for visualization)\n",
    "- **Autoencoders** (neural networks for compression)\n",
    "\n",
    "####  Example:\n",
    "From `[Age, Salary, Height, Weight]`, PCA gives you:\n",
    "- `PC1`: 70% of the variance\n",
    "- `PC2`: 20%\n",
    "- Now you use just `[PC1, PC2]` — totally new, compact features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Visual Analogy:\n",
    "\n",
    "- **Feature Selection**: Like picking the best fruits from a basket.\n",
    "- **Feature Extraction**: Like blending all the fruits into a smoothie 🍓🍌🥭 → 🥤\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Use What?\n",
    "\n",
    "| Situation                                 | Use This                            |\n",
    "|------------------------------------------|--------------------------------------|\n",
    "| You need interpretability                | ✅ Feature Selection                 |\n",
    "| You want to reduce noise or redundancy   | ✅ Both can help                     |\n",
    "| You have too many correlated features    | ✅ Feature Extraction (like PCA)     |\n",
    "| You’re feeding into models like KNN/SVM  | ✅ Feature Extraction can improve performance |\n",
    "| You want to visualize in 2D              | ✅ Feature Extraction (PCA, t-SNE)   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5d82b",
   "metadata": {},
   "source": [
    "### 14. ***What are Eigenvalues and Eigenvectors in PCA?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "#### What Are Eigenvectors and Eigenvalues?\n",
    "\n",
    "In **PCA**, eigenvectors and eigenvalues come from the **covariance matrix** of our dataset. They're what **power the transformation** to a new feature space.\n",
    "\n",
    "---\n",
    "\n",
    "#### Think of it like this:\n",
    "\n",
    "- **Eigenvectors**: 📏 The **directions** or **axes** of our new feature space (i.e., the principal components).\n",
    "- **Eigenvalues**: 📊 The **amount of variance** (information) captured along each of those directions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 In PCA terms:\n",
    "\n",
    "Let’s say I have 2D dataset. PCA will:\n",
    "1. Find the eigenvectors = the two directions in space that best describe the data’s shape.\n",
    "2. Use the eigenvalues to rank them = which direction captures more \"spread\" (variance).\n",
    "\n",
    "---\n",
    "\n",
    "#### PCA Workflow (Where Eigen Stuff Happens):\n",
    "\n",
    "1. **Standardize the data**.\n",
    "2. **Compute the covariance matrix**.\n",
    "3. **Find eigenvectors & eigenvalues** of that matrix.\n",
    "4. **Sort** eigenvectors by descending eigenvalue.\n",
    "5. **Select top `k`** eigenvectors (components).\n",
    "6. **Project data** onto these components.\n",
    "\n",
    "---\n",
    "\n",
    "#### Visualization:\n",
    "\n",
    "If the data cloud looks like an ellipse:\n",
    "\n",
    "```\n",
    "⬤      ⬤\n",
    "   ⬤       ⬤\n",
    "⬤     ⬤\n",
    "   ⬤\n",
    "```\n",
    "\n",
    "- **Eigenvector 1 (PC1)**: the **long axis** of the ellipse\n",
    "- **Eigenvalue 1**: tells us how **long/spread out** the data is along PC1\n",
    "\n",
    "- **Eigenvector 2 (PC2)**: the **shorter axis**, perpendicular to PC1\n",
    "- **Eigenvalue 2**: smaller → less variance here\n",
    "\n",
    "---\n",
    "\n",
    "#### Math Vibe:\n",
    "\n",
    "Given a square matrix **A** (like a covariance matrix):\n",
    "\n",
    "\\[\n",
    "A.v = lambda.v\n",
    "\\]\n",
    "\n",
    "- `v` is the **eigenvector**\n",
    "- `λ` (lambda) is the **eigenvalue**\n",
    "\n",
    "This equation means: when you apply matrix `A` to vector `v`, it **doesn't rotate** the vector — just **stretches or shrinks** it. That’s what makes `v` special!\n",
    "\n",
    "---\n",
    "\n",
    "#### In Practice:\n",
    "\n",
    "If PCA gives you:\n",
    "\n",
    "- Eigenvector 1: `[0.7, 0.7]` → a diagonal direction\n",
    "- Eigenvalue 1: `4.2` → explains lots of variance\n",
    "\n",
    "- Eigenvector 2: `[-0.7, 0.7]` → the other diagonal\n",
    "- Eigenvalue 2: `0.8` → not much variance\n",
    "\n",
    "We’ll **keep only the first** (eigenvector 1) if you’re reducing to 1D.\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "| Term         | What It Represents in PCA                            |\n",
    "|--------------|------------------------------------------------------|\n",
    "| Eigenvector  | A principal component (a new axis/direction)         |\n",
    "| Eigenvalue   | How much variance that component explains            |\n",
    "| Big Eigenvalue | More important component (more info retained)     |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d07af8",
   "metadata": {},
   "source": [
    "### 15. ***How do you decide the number of components to keep in PCA ?***\n",
    "*Answer-*\n",
    "\n",
    "Choosing how many **principal components** to keep in **PCA** is all about balancing **dimensionality reduction** and **information retention**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Goals When Choosing Components:**\n",
    "- **Keep most of the variance** (information)\n",
    "- **Reduce dimensionality** (fewer features → faster, simpler models)\n",
    "- **Avoid overfitting/noise**\n",
    "\n",
    "---\n",
    "\n",
    "#### **3 Common Ways to Choose the Number of Components:**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Explained Variance Ratio (Cumulative):**\n",
    "\n",
    "Each principal component explains a portion of the data's variance. You can compute the **cumulative explained variance** and pick enough components to retain, say, **95%** of the variance.\n",
    "\n",
    " → Pick the number of components where the curve **crosses 95%**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Scree Plot (Elbow Method)**\n",
    "\n",
    "A **scree plot** is a graph of eigenvalues (variance explained by each PC). We're looking for the **elbow point** — where adding more components doesn't improve much.\n",
    "\n",
    "> It’s like picking the sweet spot before the “diminishing returns” kick in.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Set `n_components` by Threshold**\n",
    "\n",
    "In `sklearn`, we can specify how much variance we want to preserve directly:\n",
    "\n",
    "```python\n",
    "pca = PCA(n_components=0.95)  # Keeps just enough PCs to retain 95% variance\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "```\n",
    "\n",
    "This automatically picks the right number of components ✅\n",
    "\n",
    "---\n",
    "\n",
    "#### Bonus: Domain Knowledge & Visualization-\n",
    "\n",
    "- Sometimes we’ll pick 2 or 3 components for **visualization**, even if that doesn’t preserve much variance.\n",
    "- Or, if we know certain features are noisy or redundant, fewer components might actually **improve performance**.\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "| Method                       | Use When…                             |\n",
    "|-----------------------------|----------------------------------------|\n",
    "| Explained Variance (95%)    | Want a clear performance tradeoff |\n",
    "| Scree Plot / Elbow          | Want to \"see\" diminishing returns |\n",
    "| `n_components=0.95` in PCA  | Want a quick, smart default        |\n",
    "| Fixed number (e.g., 2 or 3) | Doing data visualization        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a25f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1fc04c0bb20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA().fit(X_scaled)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98748d7f",
   "metadata": {},
   "source": [
    "### 16. ***Can PCA be used for classification?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "> **PCA is not a classification algorithm**, but it **can be *used with* classification** to improve performance, reduce overfitting, and help with visualization.\n",
    "\n",
    "Let’s break it down 👇\n",
    "\n",
    "---\n",
    "\n",
    "#### **What PCA Does**\n",
    "- PCA **transforms** your data to a new space by reducing its **dimensionality**.\n",
    "- It finds directions (principal components) that **maximize variance**, **not** class separation.\n",
    "\n",
    "So PCA itself doesn’t **predict labels** — it just prepares the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How PCA Helps Classification**\n",
    "\n",
    "1. **Dimensionality Reduction**  \n",
    "   - Fewer features = faster training + less risk of overfitting  \n",
    "   - Useful when you have **many features** (e.g., images, genetics)\n",
    "\n",
    "2. **Noise Reduction**  \n",
    "   - By keeping only top components, you discard **less informative/noisy features**\n",
    "\n",
    "3. **Preprocessing Step**  \n",
    "   - You can apply PCA **before feeding into** a classifier like:\n",
    "     - KNN\n",
    "     - Logistic Regression\n",
    "     - SVM\n",
    "     - Neural Networks\n",
    "\n",
    "4. **Better Performance for Some Models**  \n",
    "   - Especially useful for models sensitive to feature scaling or high dimensions (like KNN, SVM)\n",
    "\n",
    "---\n",
    "\n",
    "#### 👁️‍🗨️ PCA + Classification = Workflow Example\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### But... Watch Out!\n",
    "\n",
    "- PCA doesn’t consider class labels when reducing dimensions. So it **might not preserve class boundaries well**.\n",
    "- For classification-specific dimensionality reduction, consider:\n",
    "  - **LDA (Linear Discriminant Analysis)** → explicitly tries to **maximize class separation**\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "| Question                     | Answer                                 |\n",
    "|-----------------------------|----------------------------------------|\n",
    "| Is PCA a classifier?        | ❌ No                                  |\n",
    "| Can it help classification? | ✅ Yes — as a **preprocessing step**   |\n",
    "| Does it always improve it?  | 🔁 Depends on the dataset & labels     |\n",
    "| Better alternative?         | ✅ LDA (when class labels are known)   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ac2de",
   "metadata": {},
   "source": [
    "### 17. ***What are the limitations of PCA?***\n",
    "*Answer-*\n",
    "\n",
    "While **PCA (Principal Component Analysis)** is a powerful and widely used technique, it **does have limitations** — and knowing them helps us use it wisely.\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations of PCA**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.  **It Ignores Class Labels**\n",
    "\n",
    "- **PCA is unsupervised** — it focuses purely on **maximizing variance**, not on separating classes.\n",
    "- For classification tasks, this can lead to poor class separation.\n",
    "  - ✅ Better alternative for that? Try **LDA** (Linear Discriminant Analysis).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Assumes Linearity**\n",
    "\n",
    "- PCA assumes that the relationships between features are **linear**.\n",
    "- If your data has **non-linear structure**, PCA won't capture it well.\n",
    "  - ✅ Try **Kernel PCA** or **t-SNE/UMAP** for non-linear patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Harder to Interpret**\n",
    "\n",
    "- Principal components are **linear combinations** of original features.\n",
    "- That means the new features don’t have a **clear, intuitive meaning**.\n",
    "  - This can be a problem in domains where **interpretability is key** (like healthcare or finance).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.  **Sensitive to Feature Scaling**\n",
    "\n",
    "- PCA is very sensitive to the **scale of your features**.\n",
    "- Features with larger values (e.g., income in $1000s vs. age) will dominate the principal components.\n",
    "  - ✅ Always **standardize** your data before applying PCA.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.  **May Keep Noise**\n",
    "\n",
    "- If you choose too many components, you might **retain noisy dimensions** along with useful ones.\n",
    "- If you choose too few, you might **lose important info**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.  **Variance ≠ Importance**\n",
    "\n",
    "- PCA assumes that the **most important information = most variance**.\n",
    "- That’s not always true! Low-variance features can still be **very informative**, especially in classification.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.  **Not Ideal for Sparse Data**\n",
    "\n",
    "- PCA tends to **densify** data — even if you start with sparse input (like from TF-IDF or one-hot encodings), PCA will produce **dense components**, increasing memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.  **Not Robust to Outliers**\n",
    "\n",
    "- Outliers can heavily influence the covariance matrix → skewing principal components.\n",
    "  - ✅ Use **Robust PCA** or clean data beforehand.\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR: Should You Use PCA?\n",
    "\n",
    "| ✅ Good When...                            | ❌ Not Ideal When...                     |\n",
    "|-------------------------------------------|------------------------------------------|\n",
    "| We need to reduce dimensions for speed   | We need interpretability                |\n",
    "| We want to visualize high-dimensional data | Our data has non-linear relationships   |\n",
    "| We're preprocessing before classification | Our task is classification-focused      |\n",
    "| We have highly correlated features       | Our data has many outliers              |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d59405",
   "metadata": {},
   "source": [
    "### 18. ***How do KNN and PCA complement each other?***\n",
    "*Answer-*\n",
    "\n",
    "**`KNN** and **PCA`** work surprisingly well **together**, especially when you're dealing with **high-dimensional data**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **KNN is Sensitive to High Dimensions**\n",
    "- KNN (K-Nearest Neighbors) relies heavily on **distance calculations** (like Euclidean).\n",
    "- In high-dimensional spaces, distances become **less meaningful** due to the **curse of dimensionality** — everything starts to look equidistant!\n",
    "\n",
    "✅ **PCA helps** by **reducing the number of dimensions**, keeping only the most **informative directions**, and discarding noisy or redundant ones. This **restores meaning** to the distances KNN depends on.\n",
    "\n",
    "---\n",
    "\n",
    "####  2. **PCA Removes Noise & Redundancy**\n",
    "- KNN can suffer if features are **correlated** or **noisy**.\n",
    "- PCA helps by:\n",
    "  - **De-correlating features** (turns correlated features into orthogonal components)\n",
    "  - **Compressing variance** into fewer components\n",
    "  - Eliminating unimportant (low variance) directions\n",
    "\n",
    "✅ This makes KNN **faster**, **more accurate**, and **less prone to overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **PCA Simplifies the Feature Space**\n",
    "- KNN stores all training data — more features = **more memory + slower predictions**.\n",
    "- PCA cuts down the feature space → **leaner dataset** → **faster KNN inference**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Better Visualization & Debugging**\n",
    "- You can use PCA to **project high-dimensional data into 2D/3D**.\n",
    "- Then apply KNN and **visualize decision boundaries** or misclassified points!\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Improved Classification Performance**\n",
    "- PCA + KNN can actually **boost classification accuracy**, especially when:\n",
    "  - Your original features are **noisy**\n",
    "  - There are **too many** irrelevant features\n",
    "  - You don’t have **a lot of training data**\n",
    "\n",
    "---\n",
    "\n",
    "#### Typical Workflow:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-World Example:\n",
    "\n",
    "In image recognition (like handwritten digit classification), each image is a vector with **hundreds of pixels (features)**. PCA can reduce this to **~30–50 principal components**, then KNN can be applied **much more effectively**.\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR: Why They’re a Good Team\n",
    "\n",
    "| PCA Helps KNN By...               | Benefit 🧠                                 |\n",
    "|----------------------------------|--------------------------------------------|\n",
    "| Reducing dimensionality          | Less overfitting, faster computation       |\n",
    "| De-noising the data              | Improves accuracy                         |\n",
    "| De-correlating features          | Makes distances more meaningful           |\n",
    "| Compressing sparse data          | Speeds up prediction, saves memory        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ae2d3",
   "metadata": {},
   "source": [
    "### 19. ***How does KNN handle missing values in a dataset?***\n",
    "*Answer-*\n",
    "\n",
    "Missing values are **always tricky**, and unfortunately, **KNN (K-Nearest Neighbors)** doesn’t handle them **automatically** — but there are **clever ways to deal with them**, both **before** and **using KNN itself**.\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ KNN Classifier/Regressor Can’t Handle NaNs Directly\n",
    "\n",
    "If your dataset has missing values (e.g., `NaN`s), and you try to train or predict with `KNeighborsClassifier` or `KNeighborsRegressor` from scikit-learn:\n",
    "\n",
    "> 💥 You'll get an error:  \n",
    "> “Input contains NaN, infinity or a value too large for dtype…”\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Impute Missing Values Before Applying KNN**\n",
    "\n",
    "Use **imputation techniques** to fill in the missing values first, then run KNN.\n",
    "\n",
    "#### ➤ Simple methods:\n",
    "- Mean, median, or mode imputation\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "```\n",
    "\n",
    "#### ➤ Smarter method: **KNN Imputation**\n",
    "\n",
    "Use **KNN itself** to fill in the missing values! This is often more accurate because it considers **similar rows**.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "```\n",
    "\n",
    "- It works like this:\n",
    "  - For each missing value, it finds the **k-nearest rows (instances)** using the non-missing features\n",
    "  - Then it imputes the missing value based on those (mean for numeric, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Drop or Flag Missing Values (Less Ideal)**\n",
    "\n",
    "If there are **few missing rows**, you could:\n",
    "- **Drop** those rows/columns\n",
    "- Or add a **missing flag** (if missingness is meaningful)\n",
    "\n",
    "But for KNN, this often **loses too much data**, so imputation is better.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Table:\n",
    "\n",
    "| Method                    | What It Does                                      | Works Well When...                  |\n",
    "|--------------------------|---------------------------------------------------|-------------------------------------|\n",
    "| Simple Imputer (mean)    | Fills missing values with average                 | Data is roughly symmetric           |\n",
    "| **KNN Imputer** ✅        | Uses similar rows to estimate missing values      | Data has structure/similarity       |\n",
    "| Drop Rows/Cols           | Deletes missing data                              | Very few missing values             |\n",
    "| Missing Indicator Column | Adds binary flags for missingness                 | Missingness might carry information |\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "- **KNN classifiers/regressors don't natively handle missing values**\n",
    "- **KNN Imputation** is a smart pre-processing step that can help a lot\n",
    "- Always handle missing data **before** using KNN\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675cff3",
   "metadata": {},
   "source": [
    "### 20. ***What are the key differences between PCA and Linear Discriminant Analysis (LDA)?***\n",
    "*Answer-*\n",
    "\n",
    "PCA and LDA are both **dimensionality reduction** techniques, but they serve **very different purposes** and rely on **different mathematical principles**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### PCA vs. LDA: At a Glance\n",
    "\n",
    "| Feature                        | **PCA (Principal Component Analysis)** | **LDA (Linear Discriminant Analysis)** |\n",
    "|-------------------------------|----------------------------------------|----------------------------------------|\n",
    "| Type                          | **Unsupervised**                       | **Supervised**                         |\n",
    "| Uses class labels?            | ❌ No                                   | ✅ Yes                                  |\n",
    "| Goal                          | Maximize **overall variance**          | Maximize **class separation**          |\n",
    "| Focus                         | Global data spread                     | Inter-class vs. intra-class variance   |\n",
    "| Best For                      | Exploratory analysis, preprocessing    | Classification problems                |\n",
    "| Components ≤ #Features?       | ✅ Yes                                  | ✅ Yes                                  |\n",
    "| Components ≤ #Classes - 1?    | ❌ No                                   | ✅ Yes                                  |\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **What They Optimize**\n",
    "\n",
    "#### 🔵 PCA:\n",
    "- Finds new axes (principal components) that **maximize variance** in the data.\n",
    "- Doesn’t care about class labels — just wants the **most “spread out” directions**.\n",
    "\n",
    "#### 🔴 LDA:\n",
    "- Finds new axes that **maximize class separation** (i.e., between-class variance) while **minimizing within-class variance**.\n",
    "- It **uses the class labels** to project data for **better classification**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Dimensionality Limitation**\n",
    "\n",
    "- PCA: Can extract **up to n_features** components.\n",
    "- LDA: Can extract **up to (n_classes - 1)** components (because that’s the max number of directions you can use to separate classes).\n",
    "\n",
    "So for a 3-class classification problem, **LDA gives at most 2 components**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Visual Intuition**\n",
    "\n",
    "Imagine a 2D dataset with two classes:\n",
    "\n",
    "- **PCA** might give you the direction of **maximum spread** — even if it doesn’t help separate the classes.\n",
    "- **LDA** will find the direction that **best separates the classes**, even if the variance isn’t maximal.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **When to Use Which?**\n",
    "\n",
    "| Use **PCA** when...                            | Use **LDA** when...                         |\n",
    "|------------------------------------------------|---------------------------------------------|\n",
    "| You want to reduce dimensions **unsupervised** | You're working on a **classification task** |\n",
    "| You don’t have labels yet                      | You have labeled data                       |\n",
    "| You want to visualize the structure of data    | You want better **class separation**        |\n",
    "| You want to denoise or compress data           | You're preprocessing for classifiers        |\n",
    "\n",
    "---\n",
    "\n",
    "#### Python Example (Just to Show the Flow)\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# PCA (unsupervised)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# LDA (supervised)\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "- **PCA**: Good for reducing noise and simplifying data. Doesn’t care about labels.\n",
    "- **LDA**: Good for **classification** — finds features that best **separate classes**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e24ab4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
